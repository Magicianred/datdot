# What is DatDot

DatDot is a Distributed (peer-to-peer ) Hosting Network.

**Dynamic datasets** with immutable history - due to their P2P nature it's hard to have guarantees about their availability.
If we want to keep data available and up to date -> we have to keep our computer running or we have to rent a server.

**Challenges:** What if you want other people to help you host your data?
- what incentives do they have?
- how to find them and to trust them?
- how to verify they are seeding your data?

**Solution:**
A bridge between Dat Protocol and Substrate with a built-in incentive model which manages relationships between:
- dat creators/publishers
- dat hosters (who keep data available/host the data)

## P2P hosting protocol
When private users share their storage space with other private users.

Renting storing space is something people have been doing for many years now, but it was always through centralized hosting prividers. With the advent of the peer-to-peer technology it is now possible to directly connect private users who offer their storage with anyone who needs storage. This can be also referred to as peer-to-peer (P2P) sharing economy.

First peer-to-peer communities like Kazaa, Napster, BitTorrent have always worked directly between the peers, but with the second wave of the internet development, we saw the rise of the platforms which started to work as an intermediary. With DatDot protocol we are giving the power back to the peers and enabling them to connect directly.
![](https://i.imgur.com/S9UrzQy.png)

Users as data creators or data publishers want in the world of the new internet:
- to have full control over their data,
- their data to never be censored or taken away,
- to be able to share data over the local network and
- to be able to access it even if there is no internet available

How can we achieve that?

If there is no intermediary, there needs to be some way for peers to find each other. With the use of hypercore protocol, any data can be split in many small chunks and stored on different locations, but thanks to the protocol every peer who is storing the data, can find each other. This makes data storing and sharing trully resillient and decentralized.
![](https://i.imgur.com/KQh4clh.png)

Users in the new internet are therefore able to finally fully control their data, but still enjoy the fun and convenience of e.g. social networking or marketplaces without giving away their ownership over their accounts.

To learn more in details about how this work, read the section about the [user accounts.](Publishers (data creators))

The tehnology enables to request a specific portion of data which is served by the peer(s) who stores this particular chunk. As a reader you don't see any of this, it's all done by the technology that happens behind the scenes and enables this magical control over your own data.
![](https://i.imgur.com/WPmNNCb.png)

# Technology

## Dat (Hypercore protocol)

Dat (hypercore protocol) is a dataset synchronization protocol that does not assume a dataset is static or that the entire dataset will be downloaded. The protocol is agnostic to the underlying transport e.g. you could implement Dat (hypercore protocol) over carrier pigeon.

Its strengths are that data is hosted and distributed by many computers on the network, that it can work offline or with poor connectivity, that the original uploader can add or modify data while keeping a full history and that it can handle large amounts of data.

Dat (hypercore protocol) is a light, efficient and secure replication protocol, which makes it easy to build fast and scalable peer-to-peer applications.

The Dat storage, content integrity, and networking protocols are implemented in a module called Hypercore. Hypercore is agnostic to the format of the  input data, it operates on any stream of binary data. Think of it as an append-only log, basically just a list you can only append to. In terms of normal array operations, it is a log where you can only call get(index), push(data) and retrieve the log length, but where you can never overwrite old entries.

![](https://i.imgur.com/4StC9Aj.png)

A Hypercore's internal structure typically consist of these files:
* data: a file containing the data added to the Hypercore.
* tree: a file containing the Merkle tree of hashes derived from the data.
* signatures: a file containing the cryptographic signatures of the hashes in the tree file.
* bitfield: a file to keep track of which data we have locally, and which data is part of the network.
* public key: a file containing the public key. This is used for verifying content.
* secret key: a file containing the signing key. This is used for adding new content, and is only available on Hypercores you've created.


Hypercore allows you to quickly distribute these kind of logs in a peer-to-peer fashion. Each peer can choose to download only the section of the log they are interested in, without having to download everything from the beginning.

When files are added to te Hypercore, each file gets split up into some number of chunks, and the chunks are then arranged into a Merkle tree, which is used later for version control and replication processes.

Each chunk of data has a corresponding hash. There are also parent hashes which verify the integrity of two other hashes. Parent hashes form a tree structure.

![](https://i.imgur.com/N93l3eT.png)

Each time the author adds new chunks they calculate a root hash and sign it with the Hypercore’s secret key. Downloaders can use the Hypercore’s public key to verify the signature, which in turn verifies the integrity of all the other hashes and chunks.

Hypercore is rarely used on its own -- more powerful, multi-user data structures can be created by combining multiple cores. Hypercore's main purpose is to be a building block in other things. Some of the most used data structures are:
- Hyperdrive
- Hypertrie
- Multifeed (Kappa)

## Substrate
Substrate is an open source modular blockchain framework that lets you pick and choose the right components to build your custom chain. It comes with p2p networking, consensus algorithms, and cryptographic libraries out-of-the-box, for free.

Despite being "completely generic", it comes with both standards and conventions - particularly with the Substrate runtime module library (a.k.a FRAME) - regarding the underlying data-structures that power the STF, thereby making rapid blockchain development a reality.

When building with FRAME, the Substrate runtime is composed of several smaller components called pallets. A pallet contains a set of types, storage items, and functions that define a set of features and functionality for a runtime.

Some pallets will be sufficiently general-purpose to be reused in many blockchains. Anyone is free to write and share useful pallets. There is a collection of popular pallets provided with Substrate. Here are some of them:


>Authorship
The Autho- rship pallet tracks the current author of the block and recent uncles.

>BABE
The BABE pallet extends BABE consensus by collecting on-chain randomness from VRF outputs and managing epoch transitions.

>Balances
The Balances pallet provides functionality for handling accounts and balances.

>Collective
The Collective pallet allows a set of account IDs to make their collective feelings known through dispatched calls from specialized origins.

>Democracy
The Democracy pallet provides a democratic system that handles administration of general stakeholder voting.

>Indices
The Indices pallet allocates indices for newly created accounts. An index is a short form of an address.

>Membership
The Membership pallet allows control of membership of a set of AccountIds, useful for managing membership of a collective.

>Offences
The Offences pallet tracks reported offences.

>Randomness Collective Flip
The Randomness Collective Flip pallet provides a random function that generates low-influence random values based on the block hashes from the previous 81 blocks.

>Session
The Session pallet allows validators to manage their session keys, provides a function for changing the session length, and handles session rotation.

>Timestamp
The Timestamp pallet provides functionality to get and set the on-chain time.


### DatDot pallet
It's a custom substrate pallet which acts as a bridge between Dat (Hypercore protocol) and Substrate. It comes with a built-in incentive model which manages relationships between:
- data creators/publishers and
- data hosters

It can be used as any other pallet that comes default with Substrate. In our DatDot app we use a chain under the hood and it's requiring DatDot pallet.

To start building, check the following [How to use DatDot section](https://hackmd.io/VuD_bziuTN654YL8LE3LuA#DatDot-pallet).

# How to use DatDot

You can use DatDot in in at least 2 ways:
- as a **Dat (Hypercore protocol) user/builder** you can run it as an Electron app and use it to Host or Publish hypercore based data and/or as a gateway to the Dat world from any browser
- as a **substrate blockchain builder**, you can use it as a pallet in your chain

## DatDot pallet

To use a pallet in your chain, you'll have to follow these pretty straight forward steps:

1. Open the `your-chain/your-node/runtime/src/lib.rs`

2. **Add the external pallet to the cargo.toml**


    Because the pallet in our case is an internal pallet, we will instead use:

    `use pallet_datdot as dat_verify;`

3. **Add dat_verify as a trait to the runtime**

    ```rust
    impl dat_verify::Trait for Runtime {
        type Event = Event;
        type Randomness = RandomnessCollectiveFlip;
        type Hash = Hash;
        type ForceOrigin = pallet_collective::EnsureMember<AccountId, CouncilCollective>;
        type AttestorsPerChallenge = AttestorsPerChallenge;
        type Proposal = Call;
        type Scheduler = Scheduler;
        type ChallengeDelay = ChallengeDelay;
    }
    ```

DatDot pallet's main job is to keep a record of who is hosting what and for whom. It is also some sort of accounting manager, which keeps track of how much DatDot ratio was spent or earned. To dive deep into its functions, to find out which events are being propagated and to see how it deals with the storage, please check out its detailed [API](https://hackmd.io/lsXm88h7RVu8PmELxWMGtg).

## API

**Using Javascript** to talk to the chain and to the hypercores (dat archives)
```js
  const chainAPI = {
    newUser,
    registerHoster,
    registerEncoder,
    registerAttestor,
    publishFeedAndPlan,
    encodingDone,
    hostingStarts,
    requestProofOfStorageChallenge,
    submitProofOfStorage,
    requestAttestation,
    submitAttestationReport,
    listenToEvents,
    getFeedByID,
    getPlanByID,
    getContractByID,
    getChallengeByID,
    getAttestationByID,
    getFeedKey,
    getUserAddress,
    getHosterKey,
    getEncoderKey,
    getEncodedIndex,
  }

  const serviceAPI = {
    host,
    encode,
    getProofOfStorage,
    attest,
  }
```

**Using DatDot pallet in your chain** and call the function in rust (substrate)

**Functions:**

```rust
register_encoder(origin, noise_key: NoiseKey)

register_hoster(origin, noise_key: NoiseKey)

register_attestor(origin)

unregister_encoder(origin)

publish_feed_and_plan(origin, merkle_root: (Public, TreeHashPayload, H512), ranges: Ranges<ChunkIndex>)

encoding_done(origin, contract_id: T::ContractId)

hosting_starts(origin, contract_id: T::ContractId)

request_proof_of_storage_challenge(origin, contract_id: T::ContractId)

submit_proof_of_storage(origin, challenge_id: T::ChallengeId, proofs: Vec<Proof>0)

request_attestation(origin, contract_id: T::ContractId)

submit_attestation_report(origin, attestation_id: T::AttestationId, reports: Vec<Report> )
```

**Storage:**

```rust
GetFeedByID: map hasher(twox_64_concat) T::FeedId => Option<Feed<T>>;

GetUserByID: map hasher(twox_64_concat) T::UserId => Option<User<T>>;

GetContractByID: map hasher(twox_64_concat) T::ContractId => Option<Contract<T>>;

GetChallengeByID: map hasher(twox_64_concat) T::ChallengeId => Option<Challenge<T>>;

GetPlanByID: map hasher(twox_64_concat) T::PlanId => Option<Plan<T>>;

GetAttestationByID: map hasher(twox_64_concat) T::AttestationId => Option<Attestation<T>>;
```

## DatDot App
If you want to use DatDot as an app, visit datdot.org and click Install button to get the latest version of the app which will enable you to connect to other peers, who are also running the same protocol.

![](https://i.imgur.com/tnZetzJ.png)

DatDot app runs as a process in the background. You can decide when do you want it to be active (e.g. only at night, only over the weekends, all the time etc.) and how much resources you want to enable (e.g. bandwidth, storage etc.). You can also keep it running with recommended default setup and later adjust ot or simpl pause it when you need more resources for your work/fun.

Once you have DatDot app running on your computer, you can also access the world of DatDot OS, which make all the amazing peer-to-peer apps run inside of your favorite browser. Usually this is very tricky, since the protocol that runs in the browsers (https) doesn't support this new peer-to-peer protocol. But if you have DatDot app running, this obstacle is gone.

Everyone running the app becomes part of the DatDot network, that is why we can also call each user a peer.

![](https://i.imgur.com/G5NXC8p.png)


# Roles

## Publisher
Publisher requests the hosting and submits the data they want to have hosted.

![](https://i.imgur.com/hc0URHi.png)

- Get started
- APIs

## Hoster
Hoster is any peer who offers their available disk space to host data for the other peers. Once there is a new request for hosting, random hosters are selected and asigned parts of the data. Each hoster then stores the part of the data and makes it available to the readers (downloaders).

![](https://i.imgur.com/Oj9CRPa.png)

To proove they are indeed hosting the data, they have to solve different challenges. Only if the challenges are succesfully solved, they get rewarded for their work.

### Get started
Register as a hoster and define the resources you are willing to offer. DatDot app will regularly check if the service you provide matches your offer. Hosters who provide a reliable service, will have a priority, so be moderate with your first offer and then later (if all goes well) increase the capacities.

## Encoder
In order to make hosting service efficient and reliable, we need a role of the encoder who will compress (minimize) the data and sign the version. This process will help us check if hosters are indeed storing the data.

![](https://i.imgur.com/VI4q1s2.png)

When new request for hosting is published, a few encoders are selected. They connect to to the publisher and retreive the original data from them. In the next step, they take the data and compress it and pass the compressed data to the hosters who will be hosting it.

Encoder's role is very important and it solves two important issues:
- it compresses (minimizes) the data in order to save space on hoster's disk (this way hoster can host more data for more peers)
- it signs the compressed data with a unique signature to help the system verify if the assigned hoster is really hosting that specific set of data.


Let's go slowly through the whole process, step by step:


Step 1: When encoder receives a chunk, they compress it and they sign it.
Then they send the compressed chunk together with the signature and the merkle proof to the hoster.

![](https://i.imgur.com/NTBaELc.png)


Step 2: Hoster receives the compressed chunk from the encoder and they decompress it and use the merkle proof and decompressed chunk to perform a **Merkle verification*** (to compare it to the tree hash on chain, published by the Publisher when they request the hosting). The input in this verification is the decompressed (original) chunk and the merkle proof and the output is the tree hash that can then be compared to the tree hash on chain.

If all matches, hoster notifies the chain that they started hosting the data. On their disk they now store the compressed chunks with matching signatures and merkle proofs. They also join the swarm and serve the requested chunks to the downloaders (of course they first decompress them and then serve them).

![](https://i.imgur.com/Z9h9RDs.png)


Step 3: When the chain creates a new challenge for the hoster, the hoster needs to respond in a limited amount of time and send back a **proof-of-storage**, whch consists of 3 parts:

![](https://i.imgur.com/nW1ewA1.png)

When the chain receives the proof, it takes the public key of the encoder to verify the signature. Next, it uses Brotli to decompress the chunk and then it uses this decompressed chunk together with the merkle proof to verify the integrity of the chunk with the merkle verification.

Because data compression takes significantly longer than the decompression, hoster can not simply get the data from the swarm and quickly compress it to respond to the challenge. Next to this, they can also not sign this new compression with the encoder's private key. This means, hoster needs to actually store the data to be able to succesfully pass the challenge.

![](https://i.imgur.com/4xqriCS.png)

### Get started
Register as an encoder and offer your resources for endoding and signing chunks of data before they get hosted by the hosters. You will get rewarded for your work with the DatDot ratio.

## Attestor
Now that we are sure hoster is indeed storing the requested data chunks, we also need to make sure they are serving them to the downloaders in the swarm. This is why we also need the role of attestors who will perform the checks and report the results to the chain.

![](https://i.imgur.com/orGFVuC.png)

In regular intervals chain will select random attestor to join the swarm and fetch the requested chunk(s) of data. Attesters will report the availability and other information regarding the availability of the data (latency etc.) to the chain and will be rewarded for their work with the DatDot ratio.

### Get started
Register as an encoder and offer your resources for endoding and signing chunks of data before they get hosted by the hosters. You will get rewarded for your work with DatDot ratio.

## Validators
- Validators - verifying activity within the chain
## Datdot ratio
Every service provided by the users running the node is rewarded with DatDots. DatDots are created only when the service is performed. Publisher gets their DatDot ration reduced for the service costs (tokens get burned) and at the same time chain also creates the same amount and distributes it to the service provider.

Example: Publisher ordered hosting for their data and at the end of the time interval, Publisher's DatDots get reduced for i.e. 100 DatDots which mean the chain burns these 100 DatDots and  at the same time it also creates 100 new DatDot tokens and distributes them to the Hoster.

![](https://i.imgur.com/cgqshui.png)
